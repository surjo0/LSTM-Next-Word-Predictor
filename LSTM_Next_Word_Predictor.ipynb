{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall nltk -y\n",
        "!pip install --no-cache-dir nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooQjGM4H1rnj",
        "outputId": "61b7638e-b728-4dde-dd39-4b354f19734c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: nltk 3.9.1\n",
            "Uninstalling nltk-3.9.1:\n",
            "  Successfully uninstalled nltk-3.9.1\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nltk\n",
            "Successfully installed nltk-3.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from nltk.tokenize import word_tokenize\n"
      ],
      "metadata": {
        "id": "5O9bt-efzRbP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document = \"\"\"\n",
        "Deep Learning models have revolutionized the way we approach image classification.\n",
        "In the movie, the protagonist discovers a hidden algorithm that could change the future.\n",
        "Training a neural network requires large amounts of labeled data for accurate predictions.\n",
        "The suspense in the thriller movie built up to an intense final confrontation.\n",
        "Recurrent Neural Networks (RNNs) are often used for sequence data in machine learning.\n",
        "A good movie script can keep the audience hooked from the first scene to the last.\n",
        "Overfitting is a common challenge when training deep learning models.\n",
        "The director used clever visual effects to create a futuristic world in the movie.\n",
        "Hyperparameters play a crucial role in tuning machine learning models.\n",
        "In the movie, the AI system began to develop emotions, which led to unforeseen consequences.\n",
        "Data preprocessing is essential before feeding data into a machine learning model.\n",
        "The protagonist of the film had to navigate a virtual world controlled by AI.\n",
        "The loss function is a key component when training deep learning models.\n",
        "The plot twist in the movie was unexpected and left the audience in shock.\n",
        "Support Vector Machines (SVMs) are often used for classification problems.\n",
        "The director chose a dramatic soundtrack to accompany the action-packed scenes.\n",
        "Convolutional Neural Networks (CNNs) are primarily used for image recognition tasks.\n",
        "In the movie, the main character had to use his coding skills to outsmart the system.\n",
        "Regularization techniques help prevent overfitting in machine learning models.\n",
        "The suspenseful movie kept everyone on the edge of their seats until the final moment.\n",
        "Reinforcement learning is a type of machine learning where agents learn through interactions.\n",
        "The film explored themes of artificial intelligence and its potential dangers.\n",
        "Gradient descent is a popular optimization algorithm used in deep learning.\n",
        "The villain in the movie was a rogue AI that controlled the entire city.\n",
        "Transfer learning allows you to apply knowledge from one task to a different, related task.\n",
        "The main character of the movie had to solve a puzzle using machine learning techniques.\n",
        "Backpropagation is a technique used to train deep neural networks.\n",
        "The plot of the movie centers around a deep learning model gone rogue.\n",
        "In supervised learning, we train a model on labeled data to make predictions.\n",
        "The film's climax involved a high-stakes heist orchestrated by an AI system.\n",
        "Deep neural networks consist of multiple layers of neurons to process complex data.\n",
        "The protagonist in the movie hacked into the AI system to save the city.\n",
        "Training deep learning models on large datasets requires significant computational power.\n",
        "The movie's central theme focused on the impact of AI on human society.\n",
        "Gradient descent helps minimize the loss function in deep learning algorithms.\n",
        "The film’s setting took place in a world where humans relied heavily on machine learning.\n",
        "Neural networks can learn to perform tasks like image recognition and language translation.\n",
        "The director’s use of lighting and shadows in the movie enhanced the eerie atmosphere.\n",
        "In unsupervised learning, we aim to find patterns in data without labeled outcomes.\n",
        "The climax of the movie revealed that the AI had been manipulating human decisions.\n",
        "Data augmentation techniques help in improving the performance of machine learning models.\n",
        "The protagonist was a data scientist who uncovered a massive conspiracy involving AI.\n",
        "In the movie, the AI was trained to predict human behavior with uncanny accuracy.\n",
        "Deep learning models have achieved impressive results in natural language processing tasks.\n",
        "The final scene of the movie left the audience questioning the role of AI in the future.\n",
        "The machine learning model was trained on a large dataset of historical movie reviews.\n",
        "The villain used machine learning algorithms to predict and control future events.\n",
        "The movie featured a futuristic world where AI-controlled robots served humanity.\n",
        "The success of a machine learning model depends on the quality of the input data.\n",
        "The movie showed how AI can be both a tool for progress and a potential threat.\n",
        "Natural Language Processing (NLP) is used to analyze and interpret human language.\n",
        "The protagonist used a neural network to solve complex problems in the movie.\n",
        "Deep learning models have enabled significant advancements in voice recognition technologies.\n",
        "The plot of the movie involved an AI system that outsmarted its creators.\n",
        "The director used cutting-edge special effects to visualize machine learning algorithms in action.\n",
        "Machine learning models can make predictions based on past data and trends.\n",
        "In the movie, a group of hackers tried to manipulate an AI system for their own gain.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "cYvM0MC6zUMt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnuq_sMl0M7Y",
        "outputId": "1d600aa5-c7a0-402c-cccf-3f7a11429c80"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(document.lower())"
      ],
      "metadata": {
        "id": "EbNVtI1e2YiI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {'<unk>':0}\n",
        "\n",
        "for token in Counter(tokens).keys():\n",
        "  if token not in vocab:\n",
        "    vocab[token] = len(vocab)"
      ],
      "metadata": {
        "id": "7JZG7d9B2Z73"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucxUlfs_2f5G",
        "outputId": "9a2531e7-2040-431d-f109-0c2bc9d85b5e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<unk>': 0,\n",
              " 'deep': 1,\n",
              " 'learning': 2,\n",
              " 'models': 3,\n",
              " 'have': 4,\n",
              " 'revolutionized': 5,\n",
              " 'the': 6,\n",
              " 'way': 7,\n",
              " 'we': 8,\n",
              " 'approach': 9,\n",
              " 'image': 10,\n",
              " 'classification': 11,\n",
              " '.': 12,\n",
              " 'in': 13,\n",
              " 'movie': 14,\n",
              " ',': 15,\n",
              " 'protagonist': 16,\n",
              " 'discovers': 17,\n",
              " 'a': 18,\n",
              " 'hidden': 19,\n",
              " 'algorithm': 20,\n",
              " 'that': 21,\n",
              " 'could': 22,\n",
              " 'change': 23,\n",
              " 'future': 24,\n",
              " 'training': 25,\n",
              " 'neural': 26,\n",
              " 'network': 27,\n",
              " 'requires': 28,\n",
              " 'large': 29,\n",
              " 'amounts': 30,\n",
              " 'of': 31,\n",
              " 'labeled': 32,\n",
              " 'data': 33,\n",
              " 'for': 34,\n",
              " 'accurate': 35,\n",
              " 'predictions': 36,\n",
              " 'suspense': 37,\n",
              " 'thriller': 38,\n",
              " 'built': 39,\n",
              " 'up': 40,\n",
              " 'to': 41,\n",
              " 'an': 42,\n",
              " 'intense': 43,\n",
              " 'final': 44,\n",
              " 'confrontation': 45,\n",
              " 'recurrent': 46,\n",
              " 'networks': 47,\n",
              " '(': 48,\n",
              " 'rnns': 49,\n",
              " ')': 50,\n",
              " 'are': 51,\n",
              " 'often': 52,\n",
              " 'used': 53,\n",
              " 'sequence': 54,\n",
              " 'machine': 55,\n",
              " 'good': 56,\n",
              " 'script': 57,\n",
              " 'can': 58,\n",
              " 'keep': 59,\n",
              " 'audience': 60,\n",
              " 'hooked': 61,\n",
              " 'from': 62,\n",
              " 'first': 63,\n",
              " 'scene': 64,\n",
              " 'last': 65,\n",
              " 'overfitting': 66,\n",
              " 'is': 67,\n",
              " 'common': 68,\n",
              " 'challenge': 69,\n",
              " 'when': 70,\n",
              " 'director': 71,\n",
              " 'clever': 72,\n",
              " 'visual': 73,\n",
              " 'effects': 74,\n",
              " 'create': 75,\n",
              " 'futuristic': 76,\n",
              " 'world': 77,\n",
              " 'hyperparameters': 78,\n",
              " 'play': 79,\n",
              " 'crucial': 80,\n",
              " 'role': 81,\n",
              " 'tuning': 82,\n",
              " 'ai': 83,\n",
              " 'system': 84,\n",
              " 'began': 85,\n",
              " 'develop': 86,\n",
              " 'emotions': 87,\n",
              " 'which': 88,\n",
              " 'led': 89,\n",
              " 'unforeseen': 90,\n",
              " 'consequences': 91,\n",
              " 'preprocessing': 92,\n",
              " 'essential': 93,\n",
              " 'before': 94,\n",
              " 'feeding': 95,\n",
              " 'into': 96,\n",
              " 'model': 97,\n",
              " 'film': 98,\n",
              " 'had': 99,\n",
              " 'navigate': 100,\n",
              " 'virtual': 101,\n",
              " 'controlled': 102,\n",
              " 'by': 103,\n",
              " 'loss': 104,\n",
              " 'function': 105,\n",
              " 'key': 106,\n",
              " 'component': 107,\n",
              " 'plot': 108,\n",
              " 'twist': 109,\n",
              " 'was': 110,\n",
              " 'unexpected': 111,\n",
              " 'and': 112,\n",
              " 'left': 113,\n",
              " 'shock': 114,\n",
              " 'support': 115,\n",
              " 'vector': 116,\n",
              " 'machines': 117,\n",
              " 'svms': 118,\n",
              " 'problems': 119,\n",
              " 'chose': 120,\n",
              " 'dramatic': 121,\n",
              " 'soundtrack': 122,\n",
              " 'accompany': 123,\n",
              " 'action-packed': 124,\n",
              " 'scenes': 125,\n",
              " 'convolutional': 126,\n",
              " 'cnns': 127,\n",
              " 'primarily': 128,\n",
              " 'recognition': 129,\n",
              " 'tasks': 130,\n",
              " 'main': 131,\n",
              " 'character': 132,\n",
              " 'use': 133,\n",
              " 'his': 134,\n",
              " 'coding': 135,\n",
              " 'skills': 136,\n",
              " 'outsmart': 137,\n",
              " 'regularization': 138,\n",
              " 'techniques': 139,\n",
              " 'help': 140,\n",
              " 'prevent': 141,\n",
              " 'suspenseful': 142,\n",
              " 'kept': 143,\n",
              " 'everyone': 144,\n",
              " 'on': 145,\n",
              " 'edge': 146,\n",
              " 'their': 147,\n",
              " 'seats': 148,\n",
              " 'until': 149,\n",
              " 'moment': 150,\n",
              " 'reinforcement': 151,\n",
              " 'type': 152,\n",
              " 'where': 153,\n",
              " 'agents': 154,\n",
              " 'learn': 155,\n",
              " 'through': 156,\n",
              " 'interactions': 157,\n",
              " 'explored': 158,\n",
              " 'themes': 159,\n",
              " 'artificial': 160,\n",
              " 'intelligence': 161,\n",
              " 'its': 162,\n",
              " 'potential': 163,\n",
              " 'dangers': 164,\n",
              " 'gradient': 165,\n",
              " 'descent': 166,\n",
              " 'popular': 167,\n",
              " 'optimization': 168,\n",
              " 'villain': 169,\n",
              " 'rogue': 170,\n",
              " 'entire': 171,\n",
              " 'city': 172,\n",
              " 'transfer': 173,\n",
              " 'allows': 174,\n",
              " 'you': 175,\n",
              " 'apply': 176,\n",
              " 'knowledge': 177,\n",
              " 'one': 178,\n",
              " 'task': 179,\n",
              " 'different': 180,\n",
              " 'related': 181,\n",
              " 'solve': 182,\n",
              " 'puzzle': 183,\n",
              " 'using': 184,\n",
              " 'backpropagation': 185,\n",
              " 'technique': 186,\n",
              " 'train': 187,\n",
              " 'centers': 188,\n",
              " 'around': 189,\n",
              " 'gone': 190,\n",
              " 'supervised': 191,\n",
              " 'make': 192,\n",
              " \"'s\": 193,\n",
              " 'climax': 194,\n",
              " 'involved': 195,\n",
              " 'high-stakes': 196,\n",
              " 'heist': 197,\n",
              " 'orchestrated': 198,\n",
              " 'consist': 199,\n",
              " 'multiple': 200,\n",
              " 'layers': 201,\n",
              " 'neurons': 202,\n",
              " 'process': 203,\n",
              " 'complex': 204,\n",
              " 'hacked': 205,\n",
              " 'save': 206,\n",
              " 'datasets': 207,\n",
              " 'significant': 208,\n",
              " 'computational': 209,\n",
              " 'power': 210,\n",
              " 'central': 211,\n",
              " 'theme': 212,\n",
              " 'focused': 213,\n",
              " 'impact': 214,\n",
              " 'human': 215,\n",
              " 'society': 216,\n",
              " 'helps': 217,\n",
              " 'minimize': 218,\n",
              " 'algorithms': 219,\n",
              " '’': 220,\n",
              " 's': 221,\n",
              " 'setting': 222,\n",
              " 'took': 223,\n",
              " 'place': 224,\n",
              " 'humans': 225,\n",
              " 'relied': 226,\n",
              " 'heavily': 227,\n",
              " 'perform': 228,\n",
              " 'like': 229,\n",
              " 'language': 230,\n",
              " 'translation': 231,\n",
              " 'lighting': 232,\n",
              " 'shadows': 233,\n",
              " 'enhanced': 234,\n",
              " 'eerie': 235,\n",
              " 'atmosphere': 236,\n",
              " 'unsupervised': 237,\n",
              " 'aim': 238,\n",
              " 'find': 239,\n",
              " 'patterns': 240,\n",
              " 'without': 241,\n",
              " 'outcomes': 242,\n",
              " 'revealed': 243,\n",
              " 'been': 244,\n",
              " 'manipulating': 245,\n",
              " 'decisions': 246,\n",
              " 'augmentation': 247,\n",
              " 'improving': 248,\n",
              " 'performance': 249,\n",
              " 'scientist': 250,\n",
              " 'who': 251,\n",
              " 'uncovered': 252,\n",
              " 'massive': 253,\n",
              " 'conspiracy': 254,\n",
              " 'involving': 255,\n",
              " 'trained': 256,\n",
              " 'predict': 257,\n",
              " 'behavior': 258,\n",
              " 'with': 259,\n",
              " 'uncanny': 260,\n",
              " 'accuracy': 261,\n",
              " 'achieved': 262,\n",
              " 'impressive': 263,\n",
              " 'results': 264,\n",
              " 'natural': 265,\n",
              " 'processing': 266,\n",
              " 'questioning': 267,\n",
              " 'dataset': 268,\n",
              " 'historical': 269,\n",
              " 'reviews': 270,\n",
              " 'control': 271,\n",
              " 'events': 272,\n",
              " 'featured': 273,\n",
              " 'ai-controlled': 274,\n",
              " 'robots': 275,\n",
              " 'served': 276,\n",
              " 'humanity': 277,\n",
              " 'success': 278,\n",
              " 'depends': 279,\n",
              " 'quality': 280,\n",
              " 'input': 281,\n",
              " 'showed': 282,\n",
              " 'how': 283,\n",
              " 'be': 284,\n",
              " 'both': 285,\n",
              " 'tool': 286,\n",
              " 'progress': 287,\n",
              " 'threat': 288,\n",
              " 'nlp': 289,\n",
              " 'analyze': 290,\n",
              " 'interpret': 291,\n",
              " 'enabled': 292,\n",
              " 'advancements': 293,\n",
              " 'voice': 294,\n",
              " 'technologies': 295,\n",
              " 'outsmarted': 296,\n",
              " 'creators': 297,\n",
              " 'cutting-edge': 298,\n",
              " 'special': 299,\n",
              " 'visualize': 300,\n",
              " 'action': 301,\n",
              " 'based': 302,\n",
              " 'past': 303,\n",
              " 'trends': 304,\n",
              " 'group': 305,\n",
              " 'hackers': 306,\n",
              " 'tried': 307,\n",
              " 'manipulate': 308,\n",
              " 'own': 309,\n",
              " 'gain': 310}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPkCNe_y2iXE",
        "outputId": "f0360d7d-9258-4510-8d2b-096722b36af6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "311"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentences = document.split('\\n')"
      ],
      "metadata": {
        "id": "i9mtN_mY4Ccb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_indices(sentence, vocab):\n",
        "\n",
        "  numerical_sentence = []\n",
        "\n",
        "  for token in sentence:\n",
        "    if token in vocab:\n",
        "      numerical_sentence.append(vocab[token])\n",
        "    else:\n",
        "      numerical_sentence.append(vocab['<unk>'])\n",
        "\n",
        "  return numerical_sentence\n"
      ],
      "metadata": {
        "id": "0mFw62bJ4Hyq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_numerical_sentences = []\n",
        "\n",
        "for sentence in input_sentences:\n",
        "  input_numerical_sentences.append(text_to_indices(word_tokenize(sentence.lower()), vocab))"
      ],
      "metadata": {
        "id": "0aDjAWX94JZL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(input_numerical_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHuag6Xw4NcB",
        "outputId": "fbb8bc5d-69bb-4f57-8b72-27643e95c930"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "59"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_sequence = []\n",
        "for sentence in input_numerical_sentences:\n",
        "\n",
        "  for i in range(1, len(sentence)):\n",
        "    training_sequence.append(sentence[:i+1])"
      ],
      "metadata": {
        "id": "M1eb9b574QLy"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(training_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNDM7olZ4Uct",
        "outputId": "3620d5af-8125-40d0-9b7a-995d00c1ea46"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "761"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len_list = []\n",
        "\n",
        "for sequence in training_sequence:\n",
        "  len_list.append(len(sequence))\n",
        "\n",
        "max(len_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnoaVm0P4Xwp",
        "outputId": "96dc2546-a743-4380-a602-306c2983a2eb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_training_sequence = []\n",
        "for sequence in training_sequence:\n",
        "\n",
        "  padded_training_sequence.append([0]*(max(len_list) - len(sequence)) + sequence)"
      ],
      "metadata": {
        "id": "Qr4tg1AI4co7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_training_sequence = torch.tensor(padded_training_sequence, dtype=torch.long)"
      ],
      "metadata": {
        "id": "TP5E2qDH4h-x"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_training_sequence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0x1h2gi4mWN",
        "outputId": "bc77d227-2375-4f53-9bb4-6888825d4237"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  0,   0,   0,  ...,   0,   1,   2],\n",
              "        [  0,   0,   0,  ...,   1,   2,   3],\n",
              "        [  0,   0,   0,  ...,   2,   3,   4],\n",
              "        ...,\n",
              "        [  0,   0,  13,  ...,  34, 147, 309],\n",
              "        [  0,  13,   6,  ..., 147, 309, 310],\n",
              "        [ 13,   6,  14,  ..., 309, 310,  12]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = padded_training_sequence[:, :-1]\n",
        "y = padded_training_sequence[:,-1]"
      ],
      "metadata": {
        "id": "lHxVmcoD4pfd"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "  def __init__(self, X, y):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.X.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.X[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "M8c7M8HM4sk8"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CustomDataset(X,y)"
      ],
      "metadata": {
        "id": "zXLtT7lG4v0R"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrC4Sm1Z4yPJ",
        "outputId": "470e48f9-e35e-4eb5-c611-826ffc787836"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "761"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "op9iWfiq426u"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, 100)\n",
        "    self.lstm = nn.LSTM(100, 150, batch_first=True)\n",
        "    self.fc = nn.Linear(150, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    embedded = self.embedding(x)\n",
        "    intermediate_hidden_states, (final_hidden_state, final_cell_state) = self.lstm(embedded)\n",
        "    output = self.fc(final_hidden_state.squeeze(0))\n",
        "    return output"
      ],
      "metadata": {
        "id": "cS7PBb-a44KF"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTMModel(len(vocab))"
      ],
      "metadata": {
        "id": "BuK3pwal465f"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "learning_rate = 0.001\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "C6sY1tvg4-i2"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  total_loss = 0\n",
        "\n",
        "  for batch_x, batch_y in dataloader:\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = model(batch_x)\n",
        "\n",
        "    loss = criterion(output, batch_y)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "  print(f\"Epoch: {epoch + 1}, Loss: {total_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNqjHy1R5E4R",
        "outputId": "25ac12ef-9436-44b1-a657-e0dcb0f91f82"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 4.2468\n",
            "Epoch: 2, Loss: 3.9763\n",
            "Epoch: 3, Loss: 3.9061\n",
            "Epoch: 4, Loss: 3.9313\n",
            "Epoch: 5, Loss: 3.8883\n",
            "Epoch: 6, Loss: 3.8488\n",
            "Epoch: 7, Loss: 3.8647\n",
            "Epoch: 8, Loss: 3.8085\n",
            "Epoch: 9, Loss: 3.8693\n",
            "Epoch: 10, Loss: 3.8146\n",
            "Epoch: 11, Loss: 3.8011\n",
            "Epoch: 12, Loss: 3.8345\n",
            "Epoch: 13, Loss: 3.8639\n",
            "Epoch: 14, Loss: 3.8112\n",
            "Epoch: 15, Loss: 3.7668\n",
            "Epoch: 16, Loss: 3.8365\n",
            "Epoch: 17, Loss: 3.8997\n",
            "Epoch: 18, Loss: 3.8365\n",
            "Epoch: 19, Loss: 3.8560\n",
            "Epoch: 20, Loss: 3.7920\n",
            "Epoch: 21, Loss: 3.7898\n",
            "Epoch: 22, Loss: 3.7662\n",
            "Epoch: 23, Loss: 3.7709\n",
            "Epoch: 24, Loss: 3.7772\n",
            "Epoch: 25, Loss: 3.7995\n",
            "Epoch: 26, Loss: 3.8231\n",
            "Epoch: 27, Loss: 3.8143\n",
            "Epoch: 28, Loss: 3.7436\n",
            "Epoch: 29, Loss: 3.8549\n",
            "Epoch: 30, Loss: 3.7624\n",
            "Epoch: 31, Loss: 3.7422\n",
            "Epoch: 32, Loss: 3.7381\n",
            "Epoch: 33, Loss: 3.7274\n",
            "Epoch: 34, Loss: 3.8420\n",
            "Epoch: 35, Loss: 3.7455\n",
            "Epoch: 36, Loss: 3.7861\n",
            "Epoch: 37, Loss: 3.7877\n",
            "Epoch: 38, Loss: 3.7762\n",
            "Epoch: 39, Loss: 3.7714\n",
            "Epoch: 40, Loss: 3.7753\n",
            "Epoch: 41, Loss: 3.7671\n",
            "Epoch: 42, Loss: 3.7582\n",
            "Epoch: 43, Loss: 3.7644\n",
            "Epoch: 44, Loss: 3.7950\n",
            "Epoch: 45, Loss: 3.7259\n",
            "Epoch: 46, Loss: 3.7756\n",
            "Epoch: 47, Loss: 3.8607\n",
            "Epoch: 48, Loss: 3.7295\n",
            "Epoch: 49, Loss: 3.7679\n",
            "Epoch: 50, Loss: 3.7877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction\n",
        "def prediction(model, vocab, text):\n",
        "  tokenized_text = word_tokenize(text.lower())\n",
        "  numerical_text = text_to_indices(tokenized_text, vocab)\n",
        "  padded_text = torch.tensor([0] * (61 - len(numerical_text)) + numerical_text, dtype=torch.long).unsqueeze(0)\n",
        "  output = model(padded_text)\n",
        "  value, index = torch.max(output, dim=1)\n",
        "  return text + \" \" + list(vocab.keys())[index]"
      ],
      "metadata": {
        "id": "2ykxONjN5hlv"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "num_tokens = 10\n",
        "input_text = \"A Movie\"\n",
        "for i in range(num_tokens):\n",
        "  output_text = prediction(model, vocab, input_text)\n",
        "  print(output_text)\n",
        "  input_text = output_text\n",
        "  time.sleep(0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nTNoszn5svK",
        "outputId": "11b7513a-d4a4-4dfb-bb6b-b62fab70fdc3"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A Movie script\n",
            "A Movie script can\n",
            "A Movie script can keep\n",
            "A Movie script can keep the\n",
            "A Movie script can keep the movie\n",
            "A Movie script can keep the movie hacked\n",
            "A Movie script can keep the movie hacked into\n",
            "A Movie script can keep the movie hacked into the\n",
            "A Movie script can keep the movie hacked into the ai\n",
            "A Movie script can keep the movie hacked into the ai system\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader1 = DataLoader(dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "IE5GWMeh6jKf"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate accuracy\n",
        "def calculate_accuracy(model, dataloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in dataloader1:\n",
        "\n",
        "\n",
        "            outputs = model(batch_x)\n",
        "\n",
        "            _, predicted = torch.max(outputs, dim=1)\n",
        "\n",
        "            correct += (predicted == batch_y).sum().item()\n",
        "            total += batch_y.size(0)\n",
        "\n",
        "    accuracy = correct / total * 100\n",
        "    return accuracy\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = calculate_accuracy(model, dataloader)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QhNZonL6pwo",
        "outputId": "3a13becb-eb9c-4ada-c886-410b698e62c4"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 93.56%\n"
          ]
        }
      ]
    }
  ]
}